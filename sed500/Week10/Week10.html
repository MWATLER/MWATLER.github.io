<h1>Week 10 - Testing and Rework</h1>

<h3>Introduction</h3>
<p>This week we will look at testing and rework. This part of software development attempts to verify code functionality with 
the requirements. It also involves bug fixing.</p>

<h3>Videos</h3>
<table>
  <tr><td>Software Testing: </td><td><a href="https://www.youtube.com/watch?v=oLc9gVM8FBM" target="_blank">Software Testing Explained: How QA is Done Today</a></td></tr>
  <tr><td></td><td><a href="https://www.youtube.com/watch?v=Nd31XiSGJLw" target="_blank">What is Automated Testing?</a></td></tr>
  <tr><td></td><td><a href="https://www.youtube.com/watch?v=rq4HPm8kptg&t=49s" target="_blank">Software Maintenance in Software Engineering</a></td></tr>
</table>

<h3>Workshop(s)</h3>
<p><a href="" target="_blank"></a></p>

<h3>Assignment(s)</h3>
<p><a href="" target="_blank"></a></p>

<h3>Lecture Material</h3>
<ul>
  <li><a href="../Beginning_Software_Engineering_WholeBook.pdf" target="_blank">Beginning Software Engineering</a> Chapter 8, Testing</li>
  <li><a href="Software_Architect's_Handbook_CH6_Software_Development_Principles_and_Practices.pdf" target="_blank">Software Architect's Handbook Chapter 6</a>, Software Development Principles and Practices</li>
  <li><a href="Software_Architect's_Handbook_CH10_Performance_Considerations.pdf" target="_blank">Software Architect's Handbook Chapter 10</a>, Performance Considerations</li>
</ul>

<h3>Testing Goals</h3>

<p>Ideally you would sit down, write code that perfectly satisfies the requirements, and you'd
  be done. Unfortunately that rarely happens. More often than not, the first attempt at the
  software satisfies some but not all the requirements. It may also incorrectly handle situations
  that weren't specified in the requirements. For example, the code may not work in every
  possible situation.
  That's where testing comes in. Testing lets you study a piece of code to see whether it meets the
  requirements and whether it works correctly under all circumstances. To get a complete picture of 
  how a piece of code performs, you can carry out several different kinds of tests using a variety 
  of techniques. It is worth knowing however that it's not always
  worth removing every single bug from a program. Instead the goal is often to reduce the number
  bugs and their frequency of occurrence so that users can get their jobs done with a minimum of
  annoyance.</p>

<h4>Reasons Bugs Never Die</h4>

<p>Simply put, a bug is a flaw in a program that causes it to produce an incorrect result or to behave
  unexpectedly. Bugs are generally evil (although occasionally they make games more fun), but it's
  not always worth your effort to try to remove every bug. Removing some bugs is just more trouble
  than it's worth.</p>

<p>Finding the first few bugs in a newly written piece of software is relatively easy (and therefore
  cheap). After a few months of testing, finding bugs may become extremely difficult. At some point,
  finding the next bug would cost you more than you'll ever earn by selling the software. 
  You might delay a release to fix high-profile bugs, but if the remaining bugs aren't too bad, you
  might be forced to release before you would like.</p>

<p>Sometimes a bug fix might have undesirable consequences. For example, suppose you're building
  a drawing application and the tool that draws spirals isn't saving the spirals' colors correctly. You
  could fix it, but that would require changing the format of the saved picture files. That would force
  the users to convert their data files. In this example, it might be better to leave the spiral-saving code unfixed for now and include
  a fix in the next major release. Users expect some pain in major releases, so you might get away
  with it then.</p>

<p>If you just released a version of a program, it may be too soon to give the users a new patch to fix a
  minor bug. Users won't like you if you release new bug fixes every 3 days. As a rule of thumb:
<ul>
<li>If a bug is a security flaw, release a patch immediately, even if you just released a patch
  yesterday. (If you did release a patch yesterday, you better be sure the new patch fixes things
  correctly! Your reputation is at stake.) Include a note explaining how wonderful you are for
  protecting the users' valuable data.</li>
<li>If a bug makes users swear at your program more than once a day, release a patch as soon as
  possible (as often as monthly). Include a profuse apology.</li>
<li>If a bug is annoying enough to make users smirk at your program occasionally, fix it in a
  minor release (as often as twice a year). Include a huge fanfare about how great you are for
  looking after the users' needs.</li>
<li>If a bug is just a nice-to-have new feature or a performance improvement, fix it in the next
  major release (at most once per year). Explain how responsive you are and that the users'
  needs are your number one concern.</li>
</ul>
  Too many releases will annoy users, so you need to weigh the benefit of any bug patch against the
  inconvenience.</p>

<p>Over time, some features may become less useful. Eventually they may go away entirely. In that
  case, it may be better to just let the feature die rather than spending a lot of time fixing it.</p>

<p>Sometimes users think a feature is a bug when actually they just don't understand what the program
  is supposed to do. This is really a problem of user education. Sometimes the documentation isn't correct and
  sometimes it's missing entirely. Sometimes the user isn't willing to read all the way through both
  paragraphs of documentation and see that the feature is clearly described. If the documentation is 
  incomplete or unclear, this is a “documentation bug” that you can fix the
  next time you release a new version of the documentation. You can also greatly decrease this problem by 
  using a good user interface design. If the application
  groups features logically so users can fi nd them easily, the users won't complain that a feature
  is missing when it isn't. If features are named clearly so it's obvious what they do, users won't
  complain that a feature doesn't do what they think it's supposed to do.</p>

<p>When you fix a bug, there's a chance that you'll fix it incorrectly, so your work doesn't actually help.
  There's also a chance that you'll introduce one or more new bugs when you fix a bug.
  In fact, you're significantly more likely to add a bug to a program when you're fixing a code than
  when you're writing new code from scratch. When you write new code, you (hopefully) understand
  what you want the code to do and how it should work. When you're fixing code sometime later, you
  don't have the same level of understanding.</p>

<h4>Which Bugs to Fix</h4>

<p>ere may be some good reasons not to fix every bug, but in general bugs are bad, so you should
  remove as many of them as possible. So how do you decide which bugs to fix and which to put in the
  “fix later” category?
  To decide which bugs you should fix, you should use a simple cost/benefit analysis to prioritize
  them. For each bug, you should evaluate the following factors.
<ul>
<li>Severity —How painful is the bug for the users? How much work, time, money, or other
  resources are lost?</li>
<li>Work-arounds —Are there work-arounds?</li>
<li>Frequency —How often does the bug occur?</li>
<li>Difficulty —How hard would it be to fix the bug? (Of course, this is just a guess.)</li>
<li>Riskiness —How risky would it be to fix the bug? If the bug is in particularly complex code,
  fixing it may introduce new bugs.</li>
</ul>
  After you evaluate all the bugs, you can assign them priorities. Note that you may want the
  priorities to change over time. If your next release is a long time away, you can focus on the most
  severe bugs without work-arounds. If your time is limited, you can focus on the least risky bugs so
  that you don't break anything else before the next release.</p>

<h3>Unit Testing</h3>

<p>Testing is one of those things that some developers do not enjoy doing, even though it is
  essential to develop high-quality software applications. Developers should test early and
  often, which is a practice of many agile software development methodologies.
  If the practice of writing and executing unit tests is not already part of the organization,
  software architects should institute the practice.</p>

<p>Unit testing is the practice of testing the smallest testable units of a software system, such
  as methods, to ensure that they are working properly. It plays an important role in
  increasing the quality of your software. Functionality in a software system is decomposed
  into discrete, testable behaviors that are then tested as units.</p>

<p>Some of the principles we have discussed, such as creating loosely coupled code, following
  principles such as DRY, SoC, and single responsibility, and using techniques such as DI,
  create code that is independent and decoupled from dependencies. These qualities make
  our code testable and allow us to focus a unit test on a single unit.</p>
  
<h4>Atomic</h4>

<p>Unit tests should only test a single assumption about a small piece of functionality. This
  assumption should focus on a behavior of the unit being tested, with the unit typically
  being a single method. Therefore, multiple tests are necessary to check all of the
  assumptions for a given unit. If you are testing multiple assumptions in a single test or
  calling multiple methods in a single test, the scope of your unit test is probably too large.</p>

<h4>Isolated and independent</h4>

<p>Each unit test should have the ability to be run independently of other tests and in any
  order. If tests are isolated and independent, it should be possible to execute any unit test at
  any time.
  Unit tests should not depend on anything except the class we are testing. It should not rely
  on other classes, nor should it depend on things such as connecting to a database, using a
  hardware device, accessing files on a file system, or communicating across a network.
  With a testing framework and a DI framework, we can mock dependencies for our unit
  tests. A mock object is a simulated object that is instantiated, perhaps with the help of a
  testing/mocking framework, that mimics the behaviors of the real object. By mocking
  objects for the dependencies of the class being tested, we keep the unit test independent of
  other classes. We can control the mock and specify what it will return based on some input.</p>

<h4>Arrange</h4>

<p>In this section of the unit test method, you arrange any preconditions and inputs for the test.
  This includes initializing values and setting up mock objects.
  Depending on the unit test framework and programming language that you are using,
  some of them provide a way to specify methods that will execute prior to the execution of
  the unit tests for that test class. This allows you to centralize arrangement logic that you
  want to execute prior to the execution of all of your tests.
  However, each unit test method should have an arrange section where you are performing
  initialization for that particular test.</p>

<h4>Naming conventions for unit tests</h4>

<p>When naming your unit test classes and methods, you should follow a naming convention.
  This provides not just consistency, but allows your tests to be a form of documentation.
  Unit tests are intention-revealing in that they describe the expected behavior. If meaningful
  names are provided, everyone can know something about the purpose of the test class and
  its methods just by looking at the names.</p>

<h4>Code coverage for unit tests</h4>

<p>Code coverage is a measure of how much of the source code is being covered by tests.
  Many tools exist that will help you to calculate code coverage. Software architects should
  stress the importance of aiming for exhaustive test coverage to ensure that all code paths
  are tested.
  Keep in mind that the code coverage percentage is just part of the consideration when
  deciding whether or not you have adequate coverage. The code coverage percentage will
  let you know what percentage of the paths are covered (as in paths that are executed at
  least once). However, this does not mean that all of the important scenarios concerning
  your functionality is covered. Code coverage calculations do not consider the range of
  values that are possible for the various inputs, and additional tests may be necessary to
  cover different situations.</p>

<h4>Keeping unit tests up to date</h4>

<p>Unit tests are a form of documentation for your system, describing its functionality and the
  expected behavior. Software architects should encourage their team to not only execute unit
  tests regularly but to keep them up to date. As requirements change or new functionality is
  added, unit tests need to be changed or added.
  After changes are made, developers should modify any tests that need to be changed and
  then execute all of the unit tests to ensure there are no unintended consequences.</p>

<h3>Integration Testing</h3>

<p>After you write a chunk of code and use unit tests to verify that it works (or seems to work), it's time
  to integrate it into the existing codebase. An integration test verifit es that the new method works and
  plays well with others. It checks that existing code calls the new method correctly, and that the new
  method can call other methods correctly.
  Integration typically focuses on the new code and other pieces of code that interact with it, but it
  should also spend some time verifying that the new code didn't mess up anything that seems unrelated.</p>

<p>In regression testing, you test the program's
  entire functionality to see if anything changed when you added new code to the project. These tests
  look for ways the program may have “regressed” to a less useful version with missing or damaged
  features.
  Ideally, when you fi nish unit testing a piece of code, you would then perform integration testing to
  make sure it fits in where it should and that it didn't break anything obvious. Then you perform
  regression testing to see if it broke something non-obvious.
  Unfortunately, performing regression testing on a large project can take a lot of time, so developers
  often postpone regression testing until a signifi cant amount of code has been added or modified.
  Then they run the regression tests. Of course, at that point there may be a lot of bugs and it may be
  hard to fi gure out which change caused which bug. Some of the “new” code may also not be all that
  new, so some of the bugs may be a bit older and therefore harder to fix.
  To fix bugs as quickly as possible, you need to perform regression testing as often as possible.</p>

<h4>Automated Testing</h4>

<p>Automated testing tools let you defi ne tests and
  the results they should produce. Some of them let you record and replay keyboard events and mouse
  movements so that a test can interact with your program's user interface.
  After running a test, the testing tool can compare the results it got with expected results. Some tools
  can even compare images to see if a result is correct.</p>

<p>Some testing tools can run load tests that simulate a lot of users all running simultaneously to
  measure performance. For example, load tests can tell if too many users trying to access the same
  database will cause problems in your fi nal release.</p>
<p>A good testing tool should let you schedule tests so that you can run regression testing every night
  after the developers all go home. (Or you could start the tests running before you leave for the night.)
  When you come in the next morning, you can check the tests to see if there are any newly discovered
  problems that you should fi x before you begin writing new code.</p>

<h4>Component Interface Testing</h4>

<p>Component interface testing studies the interactions between c g omponents. This is a bit like
  regression testing in the sense that both examine the application as a whole to look for trouble, but
  component interface testing focuses on component interactions.
  A common strategy for component interface testing is to think of the interactions between
  components as one component sending a message (a request or a response) to another. You can then
  make each component record its interactions (plus a timestamp) in a fi le. To test the component
  interfaces, you exercise the system and then review the timeline of recorded events to see if
  everything makes sense.</p>

<p>Planning ahead of time for component interface testing can also help with the application's design.
  Thinking in terms of loggable messages passed between components helps keep the components
  decoupled and gives them a clearer separation. That makes them easier to implement and test
  separately.</p>

<h3>System Testing</h3>

<p>As you may guess from its name, system testing is an end-to-end run-g through of the whole system.
  Ideally, a system test exercises every part of the system to discover as many bugs as possible.
  A thorough system test may need to explore many possible paths of interaction with the application.
  Unfortunately, even simple programs usually contain a practically unlimited number of possible
  paths of interaction.</p>

<p>For example, suppose you're writing a program to keep track of dirt characteristics for hikaru
  dorodango (dirt polishing) enthusiasts, things like color, amount available, and grain size. Also
  suppose the program includes only a login screen and a single form that uses a grid to display dirt
  information. Then you would need to try each of the following operations:
<ul>
<li>Start the program and click Cancel on the login screen.
<li>Start the program, enter invalid login, click OK, verify that you get an error message, and
  finally click Cancel to close the login screen.</li>
<li>Start the program, enter invalid login, click OK, verify that you get an error message, enter
  valid login information, and click OK. Verify that you can log in.</li>
<li>Log in, view saved information, and close the program. Log in again and verify that the
  information is unchanged.</li>
<li>Log in, add new dirt information, and close the program. Login in again and verify that the
  information was saved.</li>
<li>Log in, edit some dirt information, and close the program. Login in again and verify that the
  changes were saved.</li>
<li>Log in, delete a dirt information entry, and close the program. Login in again and verify that
  the changes were saved.</li>
</ul>
You need all those tests for just two screens, neither of which can do much.
</p>

<p>For more complicated applications, the number of combinations can be enormous. In the end, you'll
  probably have to test the most common and most important scenarios, and leave some combinations
  untested.</p>

<h4>Acceptance Testing</h4>

<p>The goal of acceptance testing is to determine whether the finished application meets the customers'
  requirements. Normally, a user or other customer representative sits down with the application and
  runs through all the user cases you identified during the requirements gathering phase to make sure
  everything works as advertised.
Remember that the requirements may have changed after the requirements phase. In that case, you
  obviously verify that the application satisfi es the revised requirements.</p>
<p>Acceptance testing is usually straightforward; although, depending on the number of use cases, it can
  take a long time. A fairly simple application might need only a few use cases.
  A large, complex application with detailed needs might have dozens or
  hundreds of use cases. In that case it might take days or even weeks to go through them all.</p>
<p>One mistake developers sometimes make is waiting until the application is finished before
  starting acceptance testing. You do need to perform acceptance testing then, but if that's the first
  time the customer sees the application, there may be problems. Customers may decide that their
  interpretation of a use case is different from yours. Or they may decide that what they need is
  different from what they thought they needed during requirements gathering.
  In those cases, you're much better off if you do a quick run-through of each use case as soon as the
  application can handle it. Then if you need to change the requirements, you can do it while there's
  still some time left in the development schedule and not at the end of the project when all of the
  programmers have scheduled overseas vacations.</p>

<h3>Other Testing Categories</h3>

<p>Unit test, integration test, component interface test, and system test categorize tests based on their
  scale with unit test being at the smallest scale and system test including the entire application.
  An acceptance test differs from a system test in the point of view of the tester: A system tester is
  typically a developer or someone from product support, whereas an acceptance tester is a customer representative.
  The following list summarizes other categories of testing that differ in their scope, focus, or point
  of view.
<ul>
<li>Accessibility test —Tests the application for accessibility by those with visual, hearing, or
  other impairments.
<li>Alpha test —First round testing by selected customers or independent testers. Alpha tests
  usually uncover lots of bugs and defects, so they generally aren't open to a huge number of
  users because that might ruin your reputation for building good software.</li>
<li>Beta test—Second round testing after alpha test. Generally, you shouldn't give users beta
  versions until the application is quite solid or you might damage your reputation for building
  good software. Sometimes, beta tests are used as a sneaky form of a limited trial to build
  excitement for a new release in the user community.</li>
<li>Compatibility test —Focuses on compatibility with different environments such as computers
  running older operating system versions. Also checks compatibility with older versions of the
  application's fi les, databases, and other saved data.</li>
<li>Destructive test —Makes the application fail so that you can study its behavior when the
  worst happens. (Obviously, if you have good backups, you won't actually destroy the code.
  You'll destroy the application's performance.)</li>
<li>Functional test —Deals with features the application provides. These are generally listed in
  the requirements.</li>
<li>Installation test —Makes sure you can successfully install the system on a fresh computer.</li>
<li>Internationalization test —Tests the application on computers localized for different parts of
  the world. This should be carried out by people who are natives of the locales.</li>
<li>Nonfunctional test —Studies application characteristics that aren't related to specifi c
  functions the users will perform. For example, these tests might check performance under
  a heavy user load, with limited memory, or with missing network connections. These often
  identify minimal requirements.</li>
<li>Performance test —Studies the application's performance under various conditions such
  as normal usage, heavy user load, limited resources (such as disk space), and time of day.
  Records metrics such as the number of records processed per hour under different conditions.</li>
<li>Security test —Studies the application's security. This includes security of the login process,
  communications, and data.</li>
<li>Usability test —Determines whether the user interface is intuitive and easy to use.</li>
</ul>
</p>

<h3>Testing Techniques</h3>

<p>The previous sections described some different levels of testing (unit, integration, component,
  system, and acceptance) and alluded to some methods for testing (try out every combination of
  actions that you can think of), but they didn't explain specific techniques for performing actual tests.
  In particular, they didn't discuss generating data for tests. The following sections describe some 
  approaches to designing tests to fi nd as many bugs as possible.</p>

<h4>Exhaustive Testing</h4>

<p>In some cases, you may be able to test a method with every possible input. Such 
  exhaustive testing conclusively proves that a method works correctly under all
circumstances, so it's the best you can possibly do. Unfortunately, most methods take too many
combinations of input parameters for you to exhaustively try them all.
</p>

<p>For a ridiculously simple example where an exhaustive test is impossible, suppose you write a
  Maximum method that compares two 32-bit integers and returns the one that's larger. Each of the
  two inputs can take roughly 4.3x  109 values (between -2,147,483,648 and 2,147,483,647), so there
  are approximately 1.8 x 10 19 possible combinations. Even if you had a computer that could call the
  method and verify its results 1 billion times per second, it would take more than 570 years to check
  every combination.
  Because most methods take too many possible inputs, exhaustive testing won't work most of the
  time.</p>

<h4>Black-Box Testing</h4>

<p>In black-box testing , you pretend the method is a black box that you can't peek inside. You know
  what it is supposed to do, but you have no idea how it works. You then throw all sorts of inputs at
  the method to see what it does.</p>
<p>You can start black-box testing by sending it a bunch of random inputs. Remember that you need
  to perform these tests only occasionally, not every time the program runs, so you can test a lot of
  random values. For example, you might throw a few million random pairs of values at the Maximum
  method described in the previous section. It doesn't matter if it takes the test a few minutes to fi nish.
  Even if you don't know how the method works, you can try to guess values that might mess it up.
  Typically, those involve special values like 0 for numbers and blank for strings. They may also
  include the largest and smallest possible values. For strings that might mean a string that's all blanks
  or all ~ characters.</p>
<p>Sometimes, you can trip up a method that expects to process names by using strings containing
  numbers or special characters such as &#%!$ (which looks like a cartoon character swearing).
  Some methods don't work well if their inputs include a lot of duplicates, so try that. For example,
  quicksort is one of the fastest sorting algorithms usually, but it gives terrible performance if the
  items it is sorting all have the same value. (Consult an algorithms book or search for quicksort
  online if you want to see the details.)</p>
</p>If a method takes a variable number of inputs, make sure it can handle 0 inputs and a really large
  number of inputs. If it takes an array or list as a parameter, see what it does if the array or list is
  empty or missing.</p>

<p>Finally, look at boundary values. If a method expects a floating point parameter between 0.0 and
  1.0, make sure it can handle those two values.</p>

<h4>White-Box Testing</h4>

<p>In white-box testing, you get to know how the method does its work. You then use your extra
  knowledge to design tests to try to make the method crash and burn.
  White-box testing has the advantage that you know how the method works, so you can try to
  pick particularly difficult test cases. Unfortunately it has the disadvantage that you know how the
  method works, so you might skip some test cases that you assume work. </p>

<p>Use white-box testing to create tests you know will be troublesome, but don't skip tests that you
  “know” the method can handle.</p>

<h4>Gray-Box Testing</h4>

<p>Gray-box testing is a combination of white-box and black-box testing. Here you know some but not
  all the internals of the method you are testing. Your partial knowledge of the method lets you design
  specific tests to attack it.</p>

<p>With black-box testing, if you truly don't know how a method works, then it's
  harder to assume it handles specifi c cases correctly. Unfortunately with black-box
  testing, you don't know where to look for weaknesses.
  White-box testing lets you specifi cally attack a method's weaknesses, but as
  mentioned a couple of times (both in this chapter and in Chapter 7 ) it's easy for
  programmers to assume their code works. (That's the biggest drawback of white-box
  testing.) That can make them skip some test cases that might uncover a bug.
  You can get the best of both worlds by combining black-box and white-box testing.
  One way to do that is to have two different people test a method. The programmer
  who wrote it can build some white-box tests, and someone else can design some
  black-box tests.</p>

<h3>Pair programming</h3>

<p>Pair programming is an agile software development technique where two developers work
  together on the same deliverable, whether a technical design or coding. In the case of
  programming, the person who is coding is the driver while the other person, who is
  observing, is the navigator. The roles of driver and navigator can alternate at a prescribed
  interval (for example, every hour) or the roles can be switched at any time that the two
  people feel is appropriate. Regardless of the role, each person should be an active
  participant.</p>

<h4>Benefits of pair programming</h4>

<p>Pair programming can improve code quality. Having an additional set of eyes looking at
  the work may allow the pair to notice a problem or an opportunity that would not have
  been apparent if each person was working alone. Also, the driver will have a tendency to be
  more careful when coding with someone else watching, which may lead to better code
  overall.</p>
<p>Working collaboratively during pair programming to accomplish a goal can be beneficial. If
  the two individuals have different skillsets, they can bring both to bear on the work, and
  some of these skills will be transferred. In addition to getting work done, sessions can act
  similar to a training session in this regard. Working together also helps to enforce and
  spread knowledge of things such as coding standards.</p>
<p>Pair programming allows developers to become more familiar with the codebase. It
  provides opportunities for more than one person to be knowledgeable about a particular
  part of the system. Eventually, if something has to be changed with the code, more than one
  person will be familiar with it. Pair programming tends to create a culture of collective
  ownership of the codebase.</p>
<p>To fully realize this benefit, it is a good practice to rotate pairs and not just have the same
  two people always pair up. More knowledge will be shared when the pairs are rotated.
  Pair programming can serve as training for less experienced developers or those who may
  be new to the project. Using this technique provides an opportunity for a software architect
  or senior developer to teach someone.</p>

<h4>Code reviews</h4>

<p>Code reviews are evaluations of code, typically conducted by peers. Code reviews involve
  one or more people, other than the developer of the code being reviewed, who examine
  code changes to find any problems.</p>

<p>The main focus of code reviews is to find both technical and business logic defects.
  However, code is also reviewed for other things, such as ensuring coding standards are
  being followed and finding opportunities for improvement.</p>

<h4>Formal inspections</h4>

<p>Formal inspections, as you might imagine from the name, are a more structured way of
  reviewing deliverables. It is a group review method and has been found to be quite
  effective at finding defects. The main goal of formal inspections is to evaluate and improve
  the quality of the software system.
  Formal inspections are meetings in which deliverables, such as a design or code, are
  reviewed in order to find defects. Formal inspections are scheduled in advance, and
  participants are invited to the meeting.</p>

<h4>Walkthroughs</h4>

<p>A walkthrough is an informal method of review. In a walkthrough, the author of a design
  or code deliverable hosts a meeting in which they guide reviewers through the deliverable.
  Unlike a formal inspection, participants are not assigned specific roles (other than the
  host/author). Walkthroughs are flexible and an organization can choose how they want to
  organize a walkthrough based on their needs.</p>
<p>Participants can prepare for a walkthrough by looking at the deliverable beforehand. The
  focus of the walkthrough is to identify potential defects. Although the focus is not to correct
  any defects found, unlike formal inspections, the group can decide to allow suggestions of
  changes that can be made to the deliverable. Similar to formal inspections, management
  should not attend a walkthrough so as not to influence the meeting in any way.</p>
<p>Walkthroughs have been found to not be as effective as other review methods for
  evaluating and improving deliverables. However, they do allow a larger number of
  reviewers to participate</p>

<p></p>

<p></p>
